diff -Naur litmus-rt/include/litmus/fdso.h litmus-rt-mrsp/include/litmus/fdso.h
--- litmus-rt/include/litmus/fdso.h	2017-06-02 11:29:22.231561438 +0200
+++ litmus-rt-mrsp/include/litmus/fdso.h	2017-06-02 11:32:44.495571000 +0200
@@ -26,8 +26,9 @@
 	PCP_SEM         = 5,
 
 	DFLP_SEM	= 6,
+	MRSP_SEM	= 7,
 
-	MAX_OBJ_TYPE	= 6
+	MAX_OBJ_TYPE	= 7
 } obj_type_t;
 
 struct inode_obj_id {
diff -Naur litmus-rt/include/litmus/locking.h litmus-rt-mrsp/include/litmus/locking.h
--- litmus-rt/include/litmus/locking.h	2017-06-02 11:29:22.231561438 +0200
+++ litmus-rt-mrsp/include/litmus/locking.h	2017-06-02 11:32:44.495571000 +0200
@@ -25,4 +25,27 @@
 	void (*deallocate)(struct litmus_lock*);
 };
 
+struct mrsp_semaphore {
+	struct litmus_lock litmus_lock;
+
+	/* current resource holder */
+	struct task_struct *owner;
+	struct task_struct *helper;
+
+	/* priority queue? of waiting tasks */
+	spinlock_t lock;
+
+	/* priority ceiling per cpu */
+	int *prio_per_cpu;
+
+	/*FIFO*/
+	atomic_t serving_ticket;
+	atomic_t next_ticket;
+
+	atomic_t sem_owner_preempted;
+
+	struct cpumask resource_affinity;
+};
+
 #endif
+
diff -Naur litmus-rt/include/litmus/rt_param.h litmus-rt-mrsp/include/litmus/rt_param.h
--- litmus-rt/include/litmus/rt_param.h	2017-06-02 11:29:22.235561438 +0200
+++ litmus-rt-mrsp/include/litmus/rt_param.h	2017-06-02 11:32:44.495571000 +0200
@@ -112,6 +112,8 @@
 
 /* regular sporadic task support */
 
+struct mrsp_semaphore;
+
 struct rt_task {
 	lt_t 		exec_cost;
 	lt_t 		period;
@@ -122,6 +124,24 @@
 	task_class_t	cls;
 	budget_policy_t  budget_policy;  /* ignored by pfair */
 	release_policy_t release_policy;
+
+	struct mrsp_semaphore* mrsp_lock;
+	volatile int	ticket;
+	struct task_struct *next;
+
+	unsigned int 	saved_priority;
+	unsigned int    saved_cpu;
+	unsigned int    saved_cpu_ceiling;
+
+	/* 0-normal; 1-waiting resource; 2-resource owner */
+	volatile int	current_state;
+	volatile int	preempted_while_waiting;
+	volatile int	is_helping;
+};
+
+struct mrsp_config {
+	int prio_per_cpu[4];
+	unsigned int order ;
 };
 
 /* don't export internal data structures to user space (liblitmus) */
@@ -276,3 +296,4 @@
 #endif
 
 #endif
+
diff -Naur litmus-rt/litmus/fdso.c litmus-rt-mrsp/litmus/fdso.c
--- litmus-rt/litmus/fdso.c	2017-06-02 11:29:22.239561438 +0200
+++ litmus-rt-mrsp/litmus/fdso.c	2017-06-02 11:32:44.495571000 +0200
@@ -28,6 +28,7 @@
 	&generic_lock_ops, /* DPCP_SEM */
 	&generic_lock_ops, /* PCP_SEM */
 	&generic_lock_ops, /* DFLP_SEM */
+	&generic_lock_ops, /* MRSP_SEM */
 };
 
 static int fdso_create(void** obj_ref, obj_type_t type, void* __user config)
diff -Naur litmus-rt/litmus/fp_common.c litmus-rt-mrsp/litmus/fp_common.c
--- litmus-rt/litmus/fp_common.c	2017-06-02 11:29:22.239561438 +0200
+++ litmus-rt-mrsp/litmus/fp_common.c	2017-06-02 11:36:48.023582704 +0200
@@ -82,7 +82,8 @@
 		return 1;
 	else if (get_priority(first_task) == get_priority(second_task))
 		/* Break by PID. */
-		return first_task->pid < second_task->pid;
+		/* return first_task->pid < second_task->pid; */
+		return 0;
 	else
 		return 0;
 }
diff -Naur litmus-rt/litmus/fp_common.c~ litmus-rt-mrsp/litmus/fp_common.c~
--- litmus-rt/litmus/fp_common.c~	1970-01-01 01:00:00.000000000 +0100
+++ litmus-rt-mrsp/litmus/fp_common.c~	2017-06-02 11:30:04.791563468 +0200
@@ -0,0 +1,136 @@
+/*
+ * litmus/fp_common.c
+ *
+ * Common functions for fixed-priority scheduler.
+ */
+
+#include <linux/percpu.h>
+#include <linux/sched.h>
+#include <linux/list.h>
+
+#include <litmus/litmus.h>
+#include <litmus/sched_plugin.h>
+#include <litmus/sched_trace.h>
+
+#include <litmus/fp_common.h>
+
+/* fp_higher_prio -  returns true if first has a higher static priority
+ *                   than second. Ties are broken by PID.
+ *
+ * both first and second may be NULL
+ */
+int fp_higher_prio(struct task_struct* first,
+		   struct task_struct* second)
+{
+	struct task_struct *first_task = first;
+	struct task_struct *second_task = second;
+
+	/* There is no point in comparing a task to itself. */
+	if (unlikely(first && first == second)) {
+		TRACE_TASK(first,
+			   "WARNING: pointless FP priority comparison.\n");
+		return 0;
+	}
+
+	/* check for NULL tasks */
+	if (!first || !second)
+		return first && !second;
+
+	if (!is_realtime(second_task))
+		return 1;
+
+#ifdef CONFIG_LITMUS_LOCKING
+
+	/* Check for inherited priorities. Change task
+	 * used for comparison in such a case.
+	 */
+	if (unlikely(first->rt_param.inh_task))
+		first_task = first->rt_param.inh_task;
+	if (unlikely(second->rt_param.inh_task))
+		second_task = second->rt_param.inh_task;
+
+	/* Comparisons to itself are only possible with
+	 * priority inheritance when svc_preempt interrupt just
+	 * before scheduling (and everything that could follow in the
+	 * ready queue). Always favour the original job, as that one will just
+	 * suspend itself to resolve this.
+	 */
+	if(first_task == second_task)
+		return first_task == first;
+
+	/* Check for priority boosting. Tie-break by start of boosting.
+	 */
+	if (unlikely(is_priority_boosted(first_task))) {
+		/* first_task is boosted, how about second_task? */
+		if (is_priority_boosted(second_task))
+			/* break by priority point */
+			return lt_before(get_boost_start(first_task),
+					 get_boost_start(second_task));
+		else
+			/* priority boosting wins. */
+			return 1;
+	} else if (unlikely(is_priority_boosted(second_task)))
+		/* second_task is boosted, first is not*/
+		return 0;
+
+#else
+	/* No locks, no priority inheritance, no comparisons to itself */
+	BUG_ON(first_task == second_task);
+#endif
+
+	if (get_priority(first_task) < get_priority(second_task))
+		return 1;
+	else if (get_priority(first_task) == get_priority(second_task))
+		/* Break by PID. */
+		return first_task->pid < second_task->pid;
+	else
+		return 0;
+}
+
+int fp_ready_order(struct bheap_node* a, struct bheap_node* b)
+{
+	return fp_higher_prio(bheap2task(a), bheap2task(b));
+}
+
+void fp_domain_init(rt_domain_t* rt, check_resched_needed_t resched,
+		    release_jobs_t release)
+{
+	rt_domain_init(rt,  fp_ready_order, resched, release);
+}
+
+/* need_to_preempt - check whether the task t needs to be preempted
+ */
+int fp_preemption_needed(struct fp_prio_queue *q, struct task_struct *t)
+{
+	struct task_struct *pending;
+
+	pending = fp_prio_peek(q);
+
+	if (!pending)
+		return 0;
+	if (!t)
+		return 1;
+
+	/* make sure to get non-rt stuff out of the way */
+	return !is_realtime(t) || fp_higher_prio(pending, t);
+}
+
+void fp_prio_queue_init(struct fp_prio_queue* q)
+{
+	int i;
+
+	for (i = 0; i < FP_PRIO_BIT_WORDS; i++)
+		q->bitmask[i] = 0;
+	for (i = 0; i < LITMUS_MAX_PRIORITY; i++)
+		bheap_init(&q->queue[i]);
+}
+
+void fp_ready_list_init(struct fp_ready_list* q)
+{
+	int i;
+
+	for (i = 0; i < FP_PRIO_BIT_WORDS; i++)
+		q->bitmask[i] = 0;
+	for (i = 0; i < LITMUS_MAX_PRIORITY; i++)
+		INIT_LIST_HEAD(q->queue + i);
+}
diff -Naur litmus-rt/litmus/litmus.c litmus-rt-mrsp/litmus/litmus.c
--- litmus-rt/litmus/litmus.c	2017-06-02 11:29:22.239561438 +0200
+++ litmus-rt-mrsp/litmus/litmus.c	2017-06-02 11:32:44.495571000 +0200
@@ -161,6 +161,14 @@
 		goto out_unlock;
 	}
 
+	/* Initialize for the new variables in rt_param.h */
+	tp.mrsp_lock = NULL;
+	tp.ticket = -1;
+	tp.next = NULL;
+
+	tp.current_state = 0;
+	tp.preempted_while_waiting = 0;
+
 	target->rt_param.task_params = tp;
 
 	retval = 0;
diff -Naur litmus-rt/litmus/sched_pfp.c litmus-rt-mrsp/litmus/sched_pfp.c
--- litmus-rt/litmus/sched_pfp.c	2017-06-02 11:29:22.243561438 +0200
+++ litmus-rt-mrsp/litmus/sched_pfp.c	2017-06-02 11:32:44.495571000 +0200
@@ -199,9 +199,47 @@
 		 * release queue (if it completed). requeue() picks
 		 * the appropriate queue.
 		 */
-		if (pfp->scheduled && !blocks  && !migrate)
-			requeue(pfp->scheduled, pfp);
+		
+		/* owner is preempted */
+		if(prev && preempt && !migrate && prev->rt_param.task_params.current_state>0 && prev == prev->rt_param.task_params.mrsp_lock->owner){
+				TRACE("owner is preempted on prcessor: %d at %llu.\n", get_partition(prev) ,litmus_clock());
+			
+				prev->rt_param.task_params.cpu = -511;
+			
+				/* owner is preempted again while it is helped */
+				if(prev->rt_param.task_params.mrsp_lock->helper != NULL){
+					prev->rt_param.task_params.mrsp_lock->helper->rt_param.task_params.is_helping = 0;
+					prev->rt_param.task_params.mrsp_lock->helper = NULL;
+				}
+		}else{
+
+			if (pfp->scheduled && !blocks  && !migrate)
+				requeue(pfp->scheduled, pfp);
+		}
 		next = fp_prio_take(&pfp->ready_queue);
+
+		smp_rmb();
+		if(next != NULL && next->rt_param.task_params.current_state >0 && next == next->rt_param.task_params.mrsp_lock->owner &&
+				atomic_read(&next->rt_param.task_params.mrsp_lock->sem_owner_preempted) == 1){
+			smp_wmb();
+			atomic_set(&next->rt_param.task_params.mrsp_lock->sem_owner_preempted, 0);
+			TRACE("resume on prcessor %d.\n", get_partition(next));
+		}
+
+		smp_rmb();
+		if(prev && preempt && !migrate && prev->rt_param.task_params.current_state >0 && prev != prev->rt_param.task_params.mrsp_lock->owner){
+			TRACE("preempted while waiting on prcessor %d.\n", get_partition(prev));
+			smp_wmb();
+			prev->rt_param.task_params.preempted_while_waiting = 1;
+		}
+
+		smp_rmb();
+		if(next != NULL && next->rt_param.task_params.preempted_while_waiting == 1){
+			TRACE("resume while waiting on processor %d.\n", get_partition(next));
+			smp_wmb();
+			next->rt_param.task_params.preempted_while_waiting = 0;
+		}
+
 		if (next == prev) {
 			struct task_struct *t = fp_prio_peek(&pfp->ready_queue);
 			TRACE_TASK(next, "next==prev sleep=%d oot=%d np=%d preempt=%d migrate=%d "
@@ -249,9 +287,129 @@
 {
 	pfp_domain_t *to;
 
-	if (is_realtime(prev) &&
+	struct task_struct *helper = NULL;
+	struct task_struct *helper_1 = NULL;
+	struct task_struct *helper_2 = NULL;
+	struct task_struct *helper_3 = NULL;
+
+	unsigned long	flag;
+	int old_priority = 511;
+	int failed = 1;
+	
+	if (get_partition(prev) == -511) {
+                prev->rt_param.task_params.cpu = smp_processor_id();
+                old_priority = get_priority(prev);
+
+                spin_lock_irqsave(&prev->rt_param.task_params.mrsp_lock->lock, flag);
+                smp_mb();
+                //helper = prev->rt_param.task_params.next;     
+                /* fifo helping order */
+                helper_1 = prev->rt_param.task_params.next;
+
+                if (helper_1 != NULL){
+                        smp_rmb();
+                        if (helper_1->rt_param.task_params.preempted_while_waiting != 1){
+                                helper = helper_1;
+                                TRACE("get first help");
+                        }else{
+                                smp_mb();
+                                helper_2 = helper_1->rt_param.task_params.next;
+
+                                if (helper_2 != NULL){
+                                        smp_rmb();
+                                        if (helper_2->rt_param.task_params.preempted_while_waiting != 1){
+                                                helper = helper_2;
+                                                TRACE("get second help");
+                                        }else{
+                                                smp_mb();
+                                                helper_3 = helper_2->rt_param.task_params.next;
+                                                if (helper_3 != NULL){
+                                                        smp_rmb();
+                                                        if (helper_3->rt_param.task_params.preempted_while_waiting != 1){
+                                                                helper = helper_3;
+                                                                TRACE("get third help");
+                                                        }
+                                                }
+                                        }
+                                }
+                        }
+                }
+                spin_unlock_irqrestore(&prev->rt_param.task_params.mrsp_lock->lock, flag);
+
+                if (helper != NULL && get_partition(helper) != get_partition(prev)){
+                        to = task_pfp(helper);
+                        raw_spin_lock(&to->slock);
+
+                        prev->rt_param.task_params.cpu = get_partition(helper);
+                        prev->rt_param.task_params.priority = get_priority(helper) - 1;
+                        fp_prio_add(&to->ready_queue, prev, priority_index(prev));
+                        if (fp_preemption_needed(&to->ready_queue, to->scheduled)){
+                                helper->rt_param.task_params.is_helping = 1;
+                                prev->rt_param.task_params.mrsp_lock->helper = helper;
+                                failed = 0;
+                                preempt(to);
+                                TRACE("get help on cpu %d", get_partition(helper));
+                        }else{
+                                fp_prio_remove(&to->ready_queue, prev, priority_index(prev));
+                                BUG_ON(is_queued(prev));
+
+                                prev->rt_param.task_params.cpu = smp_processor_id();
+                                prev->rt_param.task_params.priority = old_priority;
+                                TRACE("cannot get help");
+                        }
+                        raw_spin_unlock(&to->slock);
+                }
+
+                if(failed){
+                        pfp_domain_t *pfp;
+			int failed_again = 1;
+
+                        /*check whether task can come back the original cpu*/
+                        if(prev->rt_param.task_params.cpu != prev->rt_param.task_params.saved_cpu){
+                                pfp = remote_pfp(prev->rt_param.task_params.saved_cpu);
+                                raw_spin_lock(&pfp->slock);
+                                prev->rt_param.task_params.cpu = prev->rt_param.task_params.saved_cpu;
+                                prev->rt_param.task_params.priority = prev->rt_param.task_params.saved_cpu_ceiling;
+                                fp_prio_add(&pfp->ready_queue, prev, priority_index(prev));
+
+                                if (fp_preemption_needed(&pfp->ready_queue, pfp->scheduled)){
+					
+					failed_again = 0;
+                                        preempt(pfp);
+                                        TRACE("resume to its original cpu");
+                                }else{
+					fp_prio_remove(&pfp->ready_queue, prev, priority_index(prev));
+					BUG_ON(is_queued(prev));
+					prev->rt_param.task_params.cpu = smp_processor_id();
+                                        prev->rt_param.task_params.priority = old_priority;
+                                        
+                                }
+                                raw_spin_unlock(&pfp->slock);
+                        }
+
+			if (failed_again){
+                                pfp = local_pfp;
+                                raw_spin_lock(&pfp->slock);
+                                fp_prio_add(&pfp->ready_queue, prev, priority_index(prev));
+
+                                smp_wmb();
+                                atomic_set(&prev->rt_param.task_params.mrsp_lock->sem_owner_preempted, 1);
+                                TRACE("requeue on the current cpu");
+                                raw_spin_unlock(&pfp->slock);
+                        }
+                }
+        }
+
+	else if (is_realtime(prev) &&
 	    prev->state == TASK_RUNNING &&
 	    get_partition(prev) != smp_processor_id()) {
+
+		smp_rmb();
+		if(prev->rt_param.task_params.current_state>0 && prev == prev->rt_param.task_params.mrsp_lock->owner){
+
+			return;
+		}
+
 		TRACE_TASK(prev, "needs to migrate from P%d to P%d\n",
 			   smp_processor_id(), get_partition(prev));
 
@@ -1805,6 +1963,331 @@
 	return &sem->litmus_lock;
 }
 
+/* ******************************** MRSP support ***************************** */
+
+static inline struct mrsp_semaphore* mrsp_from_lock(struct litmus_lock* lock)
+{
+	return container_of(lock, struct mrsp_semaphore, litmus_lock);
+}
+
+int pfp_mrsp_lock(struct litmus_lock* l)
+{
+	struct task_struct* t = current;
+	struct mrsp_semaphore *sem = mrsp_from_lock(l);
+	unsigned long flags;
+	unsigned int ticket;
+	struct task_struct* last_in_list = NULL;
+
+	/* save the original info before raising ceiling */
+	t->rt_param.task_params.mrsp_lock = sem;
+	t->rt_param.task_params.saved_priority = t->rt_param.task_params.priority;
+	t->rt_param.task_params.priority = sem->prio_per_cpu[get_partition(t)];
+	t->rt_param.task_params.saved_cpu = get_partition(t);
+	t->rt_param.task_params.saved_cpu_ceiling = t->rt_param.task_params.priority;
+
+	/* get the ticket */
+	spin_lock_irqsave(&sem->lock, flags);
+
+	cpumask_set_cpu(get_partition(t), &sem->resource_affinity);
+
+	smp_rmb();
+	ticket = atomic_read(&sem->next_ticket);
+	t->rt_param.task_params.ticket = ticket;
+	smp_wmb();
+	atomic_inc(&sem->next_ticket);
+	TRACE("get ticket: %d on processor %d. \n", ticket, get_partition(t));
+
+	smp_mb();
+	if(sem->owner == NULL){
+		smp_mb();
+		sem->owner = t;
+	}
+	else
+	{
+		/* fifo order */
+		smp_mb();
+		last_in_list = sem->owner;
+		smp_mb();
+		while(last_in_list->rt_param.task_params.next != NULL){
+			smp_mb();
+			last_in_list = last_in_list->rt_param.task_params.next;
+		}
+		smp_mb();
+		last_in_list->rt_param.task_params.next = t;
+		
+	}
+
+	//smp_mb();
+	//t->rt_param.task_params.next = NULL;
+
+	/* initialize the param preempted_while_waiting */
+	//smp_wmb();
+	//t->rt_param.task_params.preempted_while_waiting = 0;
+
+	/* indicate the task's current state */
+	smp_wmb();
+	t->rt_param.task_params.current_state = 1;
+	spin_unlock_irqrestore(&sem->lock, flags);
+
+	/* spin lock*/
+	while (1){
+		smp_rmb();
+		/* check the current ticket equals to the serving ticket */
+		if (atomic_read(&sem->serving_ticket) == ticket){
+			/* get the resource now */
+			smp_wmb();
+			t->rt_param.task_params.current_state = 2;
+			break;
+		}
+
+		smp_rmb();
+		if(atomic_read(&sem->sem_owner_preempted) == 1){
+			pfp_domain_t* 	pfp = local_pfp;
+
+			spin_lock_irqsave(&sem->lock, flags);
+			raw_spin_lock(&task_pfp(sem->owner)->slock);
+
+			/* check again in case of critical condition */
+			smp_rmb();
+			if(atomic_read(&sem->sem_owner_preempted) == 1){
+				struct task_struct *peak, *owner = sem->owner;
+				int old_cpu, old_priority;
+				
+				/* save all the info before operation */
+				old_cpu = get_partition(owner);
+				old_priority = get_priority(owner);
+				fp_prio_remove(&task_pfp(sem->owner)->ready_queue, owner, get_priority(owner));
+				BUG_ON(is_queued(sem->owner));
+				raw_spin_unlock(&task_pfp(sem->owner)->slock);
+
+				raw_spin_lock(&pfp->slock);
+				/* minus 1 in order to preempt the helper */
+				owner->rt_param.task_params.priority = get_priority(t) -1;
+				owner->rt_param.task_params.cpu = get_partition(t);
+				fp_prio_add(&pfp->ready_queue, owner, priority_index(owner));
+				peak = fp_prio_peek(&pfp->ready_queue);
+				raw_spin_unlock(&pfp->slock);
+
+
+				if(peak == owner){
+					smp_wmb();
+					atomic_set(&owner->rt_param.task_params.mrsp_lock->sem_owner_preempted, 0);
+
+					t->rt_param.task_params.is_helping = 1;
+					owner->rt_param.task_params.mrsp_lock->helper = t;
+					TRACE("owner is helped on %d .\n", get_partition(owner));
+					
+				}
+				else{
+					fp_prio_remove(&pfp->ready_queue, owner, get_priority(owner));
+					BUG_ON(is_queued(owner));
+					owner->rt_param.task_params.cpu = old_cpu;
+					owner->rt_param.task_params.priority = old_priority;
+					TRACE("owner cannot get help on: %d .\n", get_partition(owner));
+					
+				}
+				spin_unlock_irqrestore(&sem->lock, flags);
+				schedule();
+			}
+			else{
+				raw_spin_unlock(&task_pfp(sem->owner)->slock);
+				spin_unlock_irqrestore(&sem->lock, flags);
+			}
+
+		}
+		/* new task is on the same processor with owner, normally it won't happen */
+		if(t != sem->owner && get_partition(t) == get_partition(sem->owner))
+			schedule();
+	}
+
+	BUG_ON(sem->owner != t);
+	TRACE_TASK(t, "we got the lock on processor %d . \n", get_partition(t));
+
+	/* Increase the number of locks it holds. */
+	tsk_rt(t)->num_locks_held++;
+	return 0;
+}
+
+int pfp_mrsp_unlock(struct litmus_lock* l)
+{
+	struct task_struct *t = current;
+	struct mrsp_semaphore *sem = mrsp_from_lock(l);
+	int err = 0;
+	unsigned long flags;
+	struct task_struct *next = NULL;
+	pfp_domain_t *next_domain;
+
+	if (sem->owner != t) {
+		BUG_ON(sem->owner != t);
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* Increase the ticket so that the next waiting task can obtain the resource */
+	preempt_disable();
+	spin_lock_irqsave(&sem->lock, flags);
+	cpumask_clear_cpu(t->rt_param.task_params.saved_cpu, &sem->resource_affinity);
+
+	smp_rmb();
+	BUG_ON(atomic_read(&sem->sem_owner_preempted) == 1);
+
+	if(sem->helper != NULL){
+		sem->helper->rt_param.task_params.is_helping = 0;
+		sem->helper = NULL;
+	}
+
+	next = t->rt_param.task_params.next;
+	if(next != NULL){
+		t->rt_param.task_params.next = NULL;
+
+		next_domain = task_pfp(next);
+		raw_spin_lock(&next_domain->slock);
+
+		sem->owner = next;
+		BUG_ON(sem->owner == t);
+		
+		/* if new owner is preempted while waiting, it also need to be helped */
+		smp_rmb();
+		if(next->rt_param.task_params.preempted_while_waiting == 1){
+			if(get_partition(next) != get_partition(t)){
+				BUG_ON(!is_queued(next));
+
+				smp_wmb();
+				next->rt_param.task_params.preempted_while_waiting = 0;
+				smp_wmb();
+				atomic_set(&sem->sem_owner_preempted, 1);
+			
+			}
+			else{
+				/* next is on the same processor with prev owner, normally not happen */
+				struct task_struct *t = fp_prio_peek(&next_domain->ready_queue);
+				if(t != next ){
+					BUG_ON(!is_queued(next));
+
+					smp_wmb();
+
+					next->rt_param.task_params.preempted_while_waiting = 0;
+					smp_wmb();
+					atomic_set(&sem->sem_owner_preempted, 1);
+				
+				}
+			}
+
+		}
+
+		raw_spin_unlock(&next_domain->slock);
+	}
+	else
+		sem->owner = NULL;
+
+	smp_wmb();
+	atomic_inc(&sem->serving_ticket);
+	
+	smp_wmb();
+	t->rt_param.task_params.current_state = 0;
+
+	TRACE( "release the lock on processor %d. \n", get_partition(t));
+	spin_unlock_irqrestore(&sem->lock, flags);
+
+	/* migrate back if get help during execution */
+	if(t->rt_param.task_params.cpu != t->rt_param.task_params.saved_cpu){
+		
+		t->rt_param.task_params.cpu = t->rt_param.task_params.saved_cpu;
+		t->rt_param.task_params.priority = sem->prio_per_cpu[get_partition(t)];
+		preempt_enable();
+		schedule();
+	
+	}
+	else
+		preempt_enable();
+
+	/* lower the priority after releasing the resource */
+	t->rt_param.task_params.priority = t->rt_param.task_params.saved_priority;
+	
+	tsk_rt(t)->num_locks_held--;
+	t->rt_param.task_params.mrsp_lock = NULL;
+
+	/* call the schedule() in case there are other tasks has higher priority than the task's original task */
+	schedule();
+	
+	out:
+	return err;
+}
+
+int pfp_mrsp_open(struct litmus_lock* l, void *config)
+{
+	struct task_struct *t = current;
+	if (!is_realtime(t))
+		/* we need to know the real-time priority */
+		return -EPERM;
+	return 0;
+}
+
+int pfp_mrsp_close(struct litmus_lock* l)
+{
+	struct task_struct *t = current;
+	struct mrsp_semaphore *sem = mrsp_from_lock(l);
+	unsigned long flags;
+
+	int owner;
+
+	spin_lock_irqsave(&sem->lock, flags);
+	owner = sem->owner == t;
+	spin_unlock_irqrestore(&sem->lock, flags);
+
+	if (owner)
+		pfp_mrsp_unlock(l);
+
+	return 0;
+}
+
+void pfp_mrsp_free(struct litmus_lock* lock)
+{
+	kfree(mrsp_from_lock(lock));
+}
+
+static struct litmus_lock_ops pfp_mrsp_lock_ops = {
+	.close  = pfp_mrsp_close,
+	.lock   = pfp_mrsp_lock,
+	.open	= pfp_mrsp_open,
+	.unlock = pfp_mrsp_unlock,
+	.deallocate = pfp_mrsp_free,
+};
+
+static struct litmus_lock* pfp_new_mrsp(int* config)
+{
+	struct mrsp_semaphore* sem;
+	int i=0;
+	int *prio_per_cpu;
+	sem = kmalloc(sizeof(*sem), GFP_KERNEL);
+
+	if (!sem)
+		return NULL;
+
+	BUG_ON(!config);
+	prio_per_cpu = kmalloc(sizeof(int) * num_online_cpus(), GFP_KERNEL);
+	sem->owner   = NULL;
+	sem->helper   = NULL;
+	cpumask_clear(&sem->resource_affinity);
+	atomic_set(&sem->next_ticket, 0);
+	atomic_set(&sem->serving_ticket, 0);
+	atomic_set(&sem->sem_owner_preempted, 0);
+
+	copy_from_user (prio_per_cpu, config, sizeof(int)* num_online_cpus());
+	sem->prio_per_cpu = kmalloc(sizeof(int) * num_online_cpus(), GFP_KERNEL);
+
+	/* allocate ceilings */
+	for(i=0; i<num_online_cpus(); i++){
+		sem->prio_per_cpu[i] = prio_per_cpu[i];
+	}
+
+	spin_lock_init(&sem->lock);
+	sem->litmus_lock.ops = &pfp_mrsp_lock_ops;
+
+	return &sem->litmus_lock;
+}
+
+/* ******************************END****************************** */
 
 /* **** lock constructor **** */
 
@@ -1814,7 +2297,8 @@
 {
 	int err = -ENXIO, cpu;
 	struct srp_semaphore* srp;
-
+	int * prio_per_cpu = NULL;
+	
 	/* P-FP currently supports the SRP for local resources and the FMLP
 	 * for global resources. */
 	switch (type) {
@@ -1905,6 +2389,26 @@
 		else
 			err = -ENOMEM;
 		break;
+
+	case MRSP_SEM:
+
+		if(!config)
+			return -EFAULT;
+
+		//get_user(prio_per_cpu, (int*) config);
+		prio_per_cpu = (int*)config;
+
+		if(!prio_per_cpu)
+			return -EFAULT;
+
+		*lock = pfp_new_mrsp(prio_per_cpu);
+
+		if (*lock)
+			err = 0;
+		else
+			err = -ENOMEM;
+		break;
+
 	};
 
 	return err;
